{"cells":[{"cell_type":"markdown","id":"2c4fca33","metadata":{"id":"2c4fca33"},"source":["# [실습] OpenAI Response API\n","\n","\n","2025년 3월, OpenAI가 발표한 `Response` 인터페이스는 기존의 Chat 인터페이스를 개선한 방식입니다.   "]},{"cell_type":"code","execution_count":1,"id":"caf3f24a","metadata":{"id":"caf3f24a"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 24.3.1 -> 25.2\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["# 필수 라이브러리 설치\n","!pip install numpy pandas openai tiktoken -q"]},{"cell_type":"code","execution_count":3,"id":"afecf897","metadata":{"id":"afecf897"},"outputs":[{"name":"stdout","output_type":"stream","text":["OPENAI_API_KEY가 정상적으로 설정되어 있습니다.\n"]}],"source":["import openai\n","import os\n","\n","from dotenv import load_dotenv\n","load_dotenv(override=True)\n","\n","client = openai.OpenAI()\n","\n","# API 키 검증하기\n","try: client.models.list(); print(\"OPENAI_API_KEY가 정상적으로 설정되어 있습니다.\")\n","except: raise Exception(f\"API 키가 유효하지 않습니다!\")"]},{"cell_type":"markdown","id":"98ce68bc","metadata":{"id":"98ce68bc"},"source":["input은 기존의 messages에 해당합니다.   \n","리스트 구조가 아닌 단일 문자열로 입력하는 것도 가능합니다."]},{"cell_type":"code","execution_count":4,"id":"ddde0508","metadata":{"id":"ddde0508"},"outputs":[{"name":"stdout","output_type":"stream","text":["Response(id='resp_68907167a378819d981055dd4724ddea0f98736e1c5f60eb', created_at=1754296679.0, error=None, incomplete_details=None, instructions='이모지를 많이 넣어주세요', metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_68907167dca8819da7817e2d5c0c297b0f98736e1c5f60eb', content=[ResponseOutputText(annotations=[], text='옛날 옛적, 푸른 숲 속에는 반짝반짝 빛나는 유니콘 🦄 한 마리가 살고 있었어요. 그의 이름은 루미였죠. 루미는 무지개 🌈를 타고 달리는 것을 정말 좋아했답니다.\\n\\n어느 날 밤, 별들이 총총 빛나는 하늘 아래에서 루미는 친구들과 숨바꼭질을 했어요. 토끼 🐰, 다람쥐 🐿, 작은 부엉이 🦉가 모여 함께 웃으며 놀았죠. “나를 찾아봐!” 루미가 노래했어요.\\n\\n갑자기 숲에 촉촉한 이슬이 내리기 시작했어요. 루미는 이슬처럼 부드러운 뿔로 작은 꽃 🌼들을 보호했어요. “고마워, 루미!” 꽃들이 속삭였죠. 친구들은 유니콘과 함께 작은 둥근 바위 위에 모여, 달빛 🌕 아래에서 따뜻하게 서로를 감싸 안았답니다.\\n\\n밤이 깊어질수록 별들은 더 크게 반짝였어요 ✨. 루미와 친구들은 행복한 마음으로 눈을 감고 꿈나라로 여행을 떠났답니다. \\n\\n좋은 꿈 꾸세요! 🦄💤🌙', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=1024, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=31, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=289, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=320), user=None, store=True)\n"]}],"source":["response = client.responses.create(\n","  model=\"gpt-4.1\",\n","  input=\"Tell me a bedtime story about a unicorn. Answer in Korean.\",\n","  instructions = '이모지를 많이 넣어주세요',\n","  # 톤, 목표, 정답 예시를 포함: System Prompt와 유사\n","  max_output_tokens = 1024\n",")\n","\n","print(response)\n"]},{"cell_type":"code","execution_count":5,"id":"2bbd1258","metadata":{"id":"2bbd1258"},"outputs":[{"name":"stdout","output_type":"stream","text":["옛날 옛적, 푸른 숲 속에는 반짝반짝 빛나는 유니콘 🦄 한 마리가 살고 있었어요. 그의 이름은 루미였죠. 루미는 무지개 🌈를 타고 달리는 것을 정말 좋아했답니다.\n","\n","어느 날 밤, 별들이 총총 빛나는 하늘 아래에서 루미는 친구들과 숨바꼭질을 했어요. 토끼 🐰, 다람쥐 🐿, 작은 부엉이 🦉가 모여 함께 웃으며 놀았죠. “나를 찾아봐!” 루미가 노래했어요.\n","\n","갑자기 숲에 촉촉한 이슬이 내리기 시작했어요. 루미는 이슬처럼 부드러운 뿔로 작은 꽃 🌼들을 보호했어요. “고마워, 루미!” 꽃들이 속삭였죠. 친구들은 유니콘과 함께 작은 둥근 바위 위에 모여, 달빛 🌕 아래에서 따뜻하게 서로를 감싸 안았답니다.\n","\n","밤이 깊어질수록 별들은 더 크게 반짝였어요 ✨. 루미와 친구들은 행복한 마음으로 눈을 감고 꿈나라로 여행을 떠났답니다. \n","\n","좋은 꿈 꾸세요! 🦄💤🌙\n"]}],"source":["print(response.output[0].content[0].text)"]},{"cell_type":"markdown","id":"72abd590","metadata":{"id":"72abd590"},"source":["Response API를 사용하면, 이전의 대화를 쉽게 관리할 수 있습니다."]},{"cell_type":"code","execution_count":6,"id":"46cf223c","metadata":{"id":"46cf223c"},"outputs":[{"name":"stdout","output_type":"stream","text":["오늘은 챗지피티로\n","이야기 나누는 소중한 시간\n","마음을 나누고 즐거움을 함께하는\n","우리의 소중한 모임이여\n","--------------------------------\n","좋은 질문 감사합니다!  \n","이전에 드린 4행시는 **따뜻한 분위기**와 **챗GPT의 역할**을 표현하려고 했지만, 제시어인 '챗지피티'의 각 글자를 활용한 **정통 4행시 형식은 아니었습니다.** 그래서 구조적으로는 부족하다고 할 수 있어요.\n","\n","---\n","\n","### 🔎 스스로 평가하자면:\n","\n","- ✅ 감성: 따뜻하고 친근한 분위기는 좋았음  \n","- ❌ 형식: '챗-지-피-티' 4글자를 앞글자로 사용하지 않았음  \n","- ❌ 창의성: 다소 평범하고 예상 가능한 전개였음  \n","\n","---\n","\n","### 🎯 개선 포인트:\n","\n","- **정확한 4행시 형식 준수**\n","- **더 창의적이고 재치 있는 표현**\n","- **챗GPT의 특성을 잘 녹여낼 것**\n","\n","---\n","\n","### 🛠️ 개선된 4행시 제안:\n","\n","**챗** 하면 뭐든지 술술 나와요  \n","**지**식도 감성도 자유자재죠  \n","**피**곤할 틈 없이 신기한 이야기  \n","**티**끌 하나까지 놓치지 않는 GPT!\n","\n","---\n","\n","이 버전은 리듬감과 챗GPT의 기능적 특징(지식, 감성, 정확성)을 살려 좀 더 재미있게 구성해 봤습니다.  \n","더 다양한 스타일도 원하시면 기~꺼이 도와드릴게요! 😊\n"]}],"source":["response = client.responses.create(\n","    model=\"gpt-3.5-turbo\",\n","    input=\"챗지피티를 제시어로 4행시를 만들어 주세요.\",\n",")\n","print(response.output_text)\n","\n","print('--------------------------------')\n","\n","second_response = client.responses.create(\n","    model=\"chatgpt-4o-latest\",\n","    previous_response_id=response.id,\n","    input=[{\"role\": \"user\", \"content\": \"이 답변에 대해 좋은 시인지 스스로 평가하세요. 더 개선해 본다면?\"}],\n",")\n","print(second_response.output_text)\n"]},{"cell_type":"markdown","id":"26fdfa49","metadata":{"id":"26fdfa49"},"source":["Response API는 File Search도 지원합니다."]},{"cell_type":"code","execution_count":7,"id":"11d52215","metadata":{"id":"11d52215"},"outputs":[{"name":"stdout","output_type":"stream","text":["file-LVWtX8c7PHx6TLUyLRFmDf\n"]}],"source":["import requests\n","from io import BytesIO\n","\n","# 파일 경로로부터 OpenAI에 파일 업로드\n","def create_file(client, file_path):\n","    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n","        response = requests.get(file_path)\n","        file_content = BytesIO(response.content)\n","        file_name = file_path.split(\"/\")[-1]\n","        file_tuple = (file_name, file_content)\n","        result = client.files.create(\n","            file=file_tuple,\n","            purpose=\"assistants\"\n","        )\n","    else:\n","        with open(file_path, \"rb\") as file_content:\n","            result = client.files.create(\n","                file=file_content,\n","                purpose=\"assistants\"\n","            )\n","    print(result.id)\n","    return result.id\n","\n","\n","\n","file_id = create_file(client, \"https://cdn.openai.com/API/docs/deep_research_blog.pdf\")"]},{"cell_type":"markdown","id":"5791c4ce","metadata":{"id":"5791c4ce"},"source":["client에 업로드한 파일은 vector_store에 연결할 수 있습니다."]},{"cell_type":"code","execution_count":8,"id":"00a4cc65","metadata":{"id":"00a4cc65"},"outputs":[{"name":"stdout","output_type":"stream","text":["vs_6890724db03c8191a60eaeee7e0d5b79\n"]}],"source":["vector_store = client.vector_stores.create(\n","    name=\"deep_research_blog\"\n",")\n","print(vector_store.id)"]},{"cell_type":"markdown","id":"67b17218","metadata":{"id":"67b17218"},"source":["생성한 벡터스토어에 파일을 업로드합니다."]},{"cell_type":"code","execution_count":9,"id":"43350e87","metadata":{"id":"43350e87"},"outputs":[{"name":"stdout","output_type":"stream","text":["VectorStoreFile(id='file-LVWtX8c7PHx6TLUyLRFmDf', created_at=1754296930, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_6890724db03c8191a60eaeee7e0d5b79', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))\n"]}],"source":["result = client.vector_stores.files.create(\n","    vector_store_id=vector_store.id,\n","    file_id=file_id\n",")\n","print(result)"]},{"cell_type":"markdown","id":"e7504eb1","metadata":{"id":"e7504eb1"},"source":["파일 목록도 확인할 수 있습니다."]},{"cell_type":"code","execution_count":10,"id":"e4cf09b7","metadata":{"id":"e4cf09b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["SyncCursorPage[VectorStoreFile](data=[VectorStoreFile(id='file-LVWtX8c7PHx6TLUyLRFmDf', created_at=1754296930, last_error=None, object='vector_store.file', status='completed', usage_bytes=66539, vector_store_id='vs_6890724db03c8191a60eaeee7e0d5b79', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))], has_more=False, object='list', first_id='file-LVWtX8c7PHx6TLUyLRFmDf', last_id='file-LVWtX8c7PHx6TLUyLRFmDf')\n"]}],"source":["result = client.vector_stores.files.list(\n","    vector_store_id=vector_store.id\n",")\n","print(result)"]},{"cell_type":"markdown","id":"b3982ca5","metadata":{"id":"b3982ca5"},"source":["file_search tool을 이용해 간단한 RAG를 구현할 수 있습니다.   \n","이후의 실습에서는 LangChain을 이용해 RAG를 커스터마이징하는 방법을 배울 예정입니다."]},{"cell_type":"code","execution_count":13,"id":"66e227b5","metadata":{"id":"66e227b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Response(id='resp_6890730ea0e481a3864397bf5922ffe309783e75d1418ff0', created_at=1754297102.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseFileSearchToolCall(id='fs_6890730f0d5c81a392e452ba40eec8d809783e75d1418ff0', queries=['deep research by OpenAI'], status='completed', type='file_search_call', results=None), ResponseOutputMessage(id='msg_68907311132481a3acdd8e7e1a1015be09783e75d1418ff0', content=[ResponseOutputText(annotations=[AnnotationFileCitation(file_id='file-LVWtX8c7PHx6TLUyLRFmDf', filename='deep_research_blog.pdf', index=1159, type='file_citation'), AnnotationFileCitation(file_id='file-LVWtX8c7PHx6TLUyLRFmDf', filename='deep_research_blog.pdf', index=1159, type='file_citation'), AnnotationFileCitation(file_id='file-LVWtX8c7PHx6TLUyLRFmDf', filename='deep_research_blog.pdf', index=1159, type='file_citation'), AnnotationFileCitation(file_id='file-LVWtX8c7PHx6TLUyLRFmDf', filename='deep_research_blog.pdf', index=1159, type='file_citation'), AnnotationFileCitation(file_id='file-LVWtX8c7PHx6TLUyLRFmDf', filename='deep_research_blog.pdf', index=1159, type='file_citation')], text='OpenAI의 \"Deep Research\"는 복잡한 작업을 위해 인터넷에서 다단계 연구를 수행하는 새로운 에이전트 기능입니다. 사용자가 질문을 입력하면, Deep Research는 수백 개의 온라인 소스를 찾아 분석하고 종합하여 연구 분석가 수준의 포괄적인 보고서를 작성합니다. 이는 수많은 텍스트, 이미지, PDF를 검색, 해석, 분석하며 필요한 경우 상황에 맞게 방향을 전환할 수 있는 추론 능력을 활용합니다.\\n\\nDeep Research는 특히 금융, 과학, 정책, 공학 등 지식 집약적인 업무를 하는 사람들을 위해 설계되었으며, 까다로운 구매 결정 같은 분야에서도 맞춤형 추천을 제공할 수 있습니다. 모든 결과물에는 명확한 출처와 사고 과정을 요약해 참고와 검증이 용이하도록 문서화됩니다.\\n\\n이 기능은 OpenAI의 새로운 o3 모델(웹 브라우징과 데이터 분석에 최적화됨)을 기반으로 강화 학습을 통해 개발되었으며, 복잡하고 시간 소요가 큰 웹 연구 작업을 단 한 번의 쿼리로 신속하게 처리하여 사용자의 시간을 절약합니다.\\n\\n사용 방법은 ChatGPT에서 ‘deep research’를 선택하고 질문을 입력하면, 작업 진행 중 사이드바에 단계별 요약과 출처가 표시되고, 5분에서 30분 정도 소요되어 최종 보고서를 제공합니다. 향후 이미지, 데이터 시각화 등의 추가적 분석 결과도 포함할 예정입니다.\\n\\n제한점으로는 아직 초기 단계여서 가끔 사실이 아닌 정보를 생성하거나 부정확한 추론을 할 수 있으며, 권위 있는 정보와 루머를 구분하는 데 어려움이 있을 수 있습니다. 또한 부정확한 확신 표현, 보고서 형식 문제 등이 있을 수 있으나, 꾸준한 업데이트를 통해 개선될 예정입니다.\\n\\nDeep Research는 현재 Pro 사용자에게 우선 제공 중이며 곧 Plus 및 Team 사용자로 확대될 예정입니다. 또한 향후 내부 데이터, 구독 기반 자료 등 더 전문적인 데이터에 접근하여 더욱 견고하고 개인화된 결과를 낼 수 있도록 발전할 계획입니다.\\n\\n요약하자면, OpenAI의 Deep Research는 인터넷 전반에서 정보를 탐색하고 종합하여 깊이 있는, 신뢰할 수 있는 연구 보고서를 자동으로 작성하는 지능형 연구 도구입니다. 이는 기존 ChatGPT보다 훨씬 더 심도 있는 조사와 분석을 가능하게 하여 학술 연구, 비즈니스 분석 등 다양한 분야에서 활용할 수 있습니다.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FileSearchTool(type='file_search', vector_store_ids=['vs_6890724db03c8191a60eaeee7e0d5b79'], filters=None, max_num_results=20, ranking_options=RankingOptions(ranker='auto', score_threshold=0.0))], top_p=1.0, background=False, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=17384, input_tokens_details=InputTokensDetails(cached_tokens=9454), output_tokens=619, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18003), user=None, store=True)\n"]}],"source":["response = client.responses.create(\n","    model=\"gpt-4.1-mini\",\n","    input=\"What is deep research by OpenAI? Please answer in Korean.\",\n","    tools=[{\n","        \"type\": \"file_search\",\n","        \"vector_store_ids\": [vector_store.id]\n","        # \"max_num_results\": 2 # 검색 결과 최대 개수\n","    }],\n","    # include=[\"file_search_call.results\"] # 검색 결과 포함\n",")\n","print(response)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":5}
